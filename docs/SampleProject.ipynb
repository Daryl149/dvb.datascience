{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample project based on IRIS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create a sample project, setting up a pipeline to train a model and test this model on new data.\n",
    "First we import dvb.datascience to start our project.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvb.datascience as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pipeline, and read in the standard IRIS dataset included in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ds.Pipeline()\n",
    "p.addPipe(\"read\", ds.data.SampleData(dataset_name=\"iris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data to take a look at what we have.\n",
    "To do that, we add a pipe to split our data, with a specified percentage to be stored as test data, and another pipe to perform the actual scatter plots. We have to define where the previous pipe originates from, hence the [(\"read\", \"df\", \"df\")] when adding the split pipe. After adding the necessary pipes, we fit and transform the train data, and transform the test data to display our scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.addPipe('split', ds.transform.RandomTrainTestSplit(test_size=0.3), [(\"read\", \"df\", \"df\")]) \n",
    "p.addPipe('scatter', ds.eda.ScatterPlots(), [(\"split\", \"df\", \"df\")]) \n",
    "p.fit_transform(transform_params={'split': {'split': ds.transform.split.TrainTestSplitBase.TRAIN}})\n",
    "p.transform(transform_params={'split': {'split': ds.transform.split.TrainTestSplitBase.TEST}}, name='test', close_plt=True)\n",
    "p.get_pipe_output(\"read\") # we can use the function get_pipe_output(name_pipe) to read the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline has now been used, so we cannot insert the same names for the pipes. We will set up a new pipeline so we can display all the possibilities and to train the data using a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ds.Pipeline()\n",
    "p.addPipe(\"read\", ds.data.SampleData(dataset_name=\"iris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to filter out certain observations, drop whole columns or add new labels, we can add the following pipes.\n",
    "\n",
    "In the add_total_petal_size pipe, we add a new column to every row.\n",
    "\n",
    "In the target_0 pipe we only include the rows which satisfy a condition, in this case row[\"target\"] == 0.\n",
    "\n",
    "In the drop_petal_length pipe, we drop the petal_length column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_size(row):\n",
    "    total_size = float(row[\"sepal length (cm)\"]) * float(row[\"sepal width (cm)\"])\n",
    "    return total_size\n",
    "\n",
    "p.addPipe(\n",
    "    \"add_total_sepal_size\",\n",
    "    ds.transform.ComputeFeature(\"total_size\",lambda row: get_total_size(row)),\n",
    "    [(\"read\", \"df\", \"df\")],\n",
    ")\n",
    "\n",
    "p.addPipe(\n",
    "    \"target_0\", # Note: this pipe includes the observations which result in True\n",
    "    ds.transform.FilterObservations(lambda row: row[\"target\"] == 0 ),\n",
    "    [(\"add_total_sepal_size\", \"df\", \"df\")],\n",
    ")\n",
    "\n",
    "p.addPipe(\n",
    "        \"drop_petal_length\",\n",
    "        ds.transform.DropFeatures(\n",
    "            [\"petal length (cm)\"]\n",
    "        ),\n",
    "        [(\"target_0\", \"df\", \"df\")],\n",
    "    )\n",
    "p.fit_transform()\n",
    "p.get_pipe_output(\"drop_petal_length\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to actually try and predict some data. This is a full pipeline from reading to generating a model. \n",
    "\n",
    "To preface some things: Multi-Class predictions are not yet implemented, as well as regression models. Only classifiers and binary predictions are available as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import dvb.datascience as ds # Import our code and sklearn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "p = ds.Pipeline() # Generate a new pipeline\n",
    "p.addPipe(\"read\", ds.data.SampleData(dataset_name=\"iris\")) # Read in your data, in this case SampleData Iris.\n",
    "\n",
    "# If you want to read from a datafile, you can use the following pipe:\n",
    "# p.addPipe(\"read\", ds.data.CSVDataImportPipe(filename, index_col=index_column_to_be_used))\n",
    "\n",
    "# We add a metadata pipeline, so we can have a prediction target.\n",
    "p.addPipe(\"metadata\", ds.data.DataPipe(\"df_metadata\", {\"y_true_label\": \"target\"}))\n",
    "\n",
    "# Split our data in training and test size, in this case 70% training, 30% test data.\n",
    "p.addPipe(\n",
    "    \"split\",\n",
    "    ds.transform.RandomTrainTestSplit(test_size=0.3),\n",
    "    inputs=[(\"read\", \"df\", \"df\")],\n",
    ")\n",
    "\n",
    "# As mentioned, we only support binary labels as of now, so we filter out one of the three targets.\n",
    "p.addPipe(\n",
    "    \"target_0_1\", # Note: this pipe includes the rows which result in True and discards rows which result in False\n",
    "    ds.transform.FilterObservations(lambda row: row[\"target\"] == 0 or row[\"target\"] == 1 ),\n",
    "    [(\"split\", \"df\", \"df\")],\n",
    ")\n",
    "\n",
    "# Now lets say that we want to normalize our values. We don't want to normalize the values for our targets,\n",
    "# so we remove these temporarily from our data, only to merge them back later.\n",
    "p.addPipe(\n",
    "    \"remove_label\", ds.transform.DropFeatures([\"target\"]), [(\"target_0_1\", \"df\", \"df\")]\n",
    ")\n",
    "p.addPipe(\n",
    "    \"keep_label\", ds.transform.FilterFeatures([\"target\"]), [(\"target_0_1\", \"df\", \"df\")]\n",
    ")\n",
    "p.addPipe(\n",
    "    \"scaler\",\n",
    "    ds.transform.SKLearnWrapper(StandardScaler),\n",
    "    [(\"remove_label\", \"df\", \"df\")],\n",
    ")\n",
    "p.addPipe(\n",
    "    \"merge\",\n",
    "    ds.transform.Union(2, \"inner\"),\n",
    "    [(\"keep_label\", \"df\", \"df0\"), (\"scaler\", \"df\", \"df1\")],\n",
    ")\n",
    "\n",
    "# We insert a correlation matrix, to give us a better overview of our data\n",
    "p.addPipe(\"corrmatrix\", ds.eda.CorrMatrixPlot(), inputs=[(\"merge\", \"df\", \"df\")])\n",
    "\n",
    "# We use a LogisticRegression classifier in this model\n",
    "p.addPipe(\n",
    "    \"LogisticRegression\",\n",
    "    ds.predictor.SklearnClassifier(\n",
    "        LogisticRegression,\n",
    "    ),\n",
    "    [\n",
    "        (\"merge\", \"df\", \"df\"),\n",
    "        (\"metadata\", \"df_metadata\", \"df_metadata\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display the score so we can measure it\n",
    "p.addPipe(\n",
    "    \"LogisticRegression_score\",\n",
    "    ds.score.ClassificationScore(),\n",
    "    [(\"LogisticRegression\", \"predict\", \"predict\"), (\"LogisticRegression\", \"predict_metadata\", \"predict_metadata\")],\n",
    ")\n",
    "\n",
    "# We fit our data on our model, using the TRAIN data set which we split earlier\n",
    "p.fit_transform(\n",
    "    name=\"train\",\n",
    "    transform_params={\n",
    "        \"split\": {\"split\": ds.transform.split.TrainTestSplitBase.TRAIN},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Here we test our model, using our TEST data set\n",
    "p.transform(\n",
    "    name=\"validate\",\n",
    "    transform_params={\n",
    "        \"split\": {\"split\": ds.transform.split.TrainTestSplitBase.TEST},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats a basic model. You can add a lot more different pipes, split your data on more than TRAIN/TEST, use different classifiers and visualize these using the plotting pipes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
